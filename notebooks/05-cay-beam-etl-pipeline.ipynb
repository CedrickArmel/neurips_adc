{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-10 10:58:34.313078: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-10 10:58:34.315301: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-10-10 10:58:34.346901: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-10 10:58:34.346930: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-10 10:58:34.347754: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-10 10:58:34.352700: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-10-10 10:58:34.353247: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-10 10:58:35.227871: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import storage\n",
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import PipelineOptions, StandardOptions\n",
    "from apache_beam.runners.interactive.interactive_runner import InteractiveRunner\n",
    "import apache_beam.runners.interactive.interactive_beam as ibeam\n",
    "import os\n",
    "import subprocess\n",
    "import glob\n",
    "import shutil\n",
    "from typing import Union\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from typing import List, Tuple, Any, Dict\n",
    "from astropy.stats import sigma_clip\n",
    "import itertools\n",
    "from gcsfs import GCSFileSystem\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONTENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_blobs(bucket_name: str, folder: str | None = None) -> list[str]:\n",
    "    \"\"\"List the object in the bucket (folder).\n",
    "\n",
    "    Args:\n",
    "        bucket_name (str): Bucket to consider.\n",
    "        folder (str | None, optional): Folder to whose elements to list. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: Listed objects' uris.\n",
    "    \"\"\"\n",
    "    storage_client = storage.Client()\n",
    "    blobs = storage_client.list_blobs(bucket_name)\n",
    "    if folder:\n",
    "        folder_objects = []\n",
    "        for blob in blobs:\n",
    "            if blob.name.startswith(folder):\n",
    "                folder_objects.append(\"gs://\" + bucket_name + \"/\" + blob.name)\n",
    "        return folder_objects\n",
    "    else:\n",
    "        return [\"gs://\" + bucket_name + \"/\" + blob.name for blob in blobs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_key(key_value: List[Tuple[Any, Any]]) -> List[List[Any]]:\n",
    "    \"\"\"\n",
    "    Group a list of key-value pairs by key.\n",
    "\n",
    "    Args:\n",
    "        key_value (List[Tuple[Any, Any]]): List of tuples where each tuple contains a key and a value.\n",
    "\n",
    "    Returns:\n",
    "        List[List[Any]]: A list of lists where each sublist contains the key and a list of associated values.\n",
    "    \"\"\"\n",
    "    grouped_data = defaultdict(list)\n",
    "    for key, value in key_value:\n",
    "        grouped_data[key].append(value)\n",
    "    return [[key, values] for key, values in grouped_data.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_key_value(path: str):\n",
    "    l = path.split(\"/\")\n",
    "    if len(l) <= 5:\n",
    "        return (l[3], \"/\".join(l) if l[4] != \"\" else None)\n",
    "    else:\n",
    "        return (l[5], \"/\".join(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_data_uris(\n",
    "    bucket_name: str, folder: str | None = None\n",
    ") -> List[Tuple[str, List[str]]]:\n",
    "    \"\"\"\n",
    "    Get the data URIs in the Google Cloud Storage bucket.\n",
    "\n",
    "    Args:\n",
    "        bucket_name (str): Bucket to consider.\n",
    "        floder (str): Folder to whose elements to list. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[str, List[str]]]: List of tuples where each tuple has the planet ID as the key and its data as the value.\n",
    "    \"\"\"\n",
    "    paths = list_blobs(bucket_name, folder)\n",
    "    key_value_pairs = [split_key_value(path=path) for path in paths]\n",
    "    raw_data = [\n",
    "        item[1] for item in key_value_pairs if item[0] == \"raw\" and item[1] is not None\n",
    "    ]\n",
    "    non_raw_data = [item for item in key_value_pairs if item[0] != \"raw\"]\n",
    "    grouped_data = group_by_key(non_raw_data)\n",
    "    result = [(key, value + raw_data) for key, value in grouped_data]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_tfrecords(dataset: tf.data.Dataset, path: str):\n",
    "    with tf.io.TFRecordWriter(path) as file_writer:\n",
    "        for record in dataset:\n",
    "            try:\n",
    "                file_writer.write(record.numpy())\n",
    "            except AttributeError:\n",
    "                file_writer.write(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_fn(proto):\n",
    "    return tf.io.parse_tensor(proto, out_type=tf.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tfrecord(path: str):\n",
    "    return tf.data.TFRecordDataset(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_records(dataset: tf.data.Dataset):\n",
    "    for record in dataset:\n",
    "        print(record.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deserialize_example(ex: tf.train.Example, formats: dict, decoders: dict | None):\n",
    "    example = tf.io.parse_single_example(example, formats)\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_labeled_tfrecord(example):\n",
    "    LABELED_TFREC_FORMAT = {\n",
    "        \"id\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"airs\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"fgs\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"target\": tf.io.FixedLenFeature([], tf.string),\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n",
    "    id = example[\"id\"]\n",
    "    airs = decode_fn(example[\"airs\"])\n",
    "    fgs = decode_fn(example[\"fgs\"])\n",
    "    target = decode_fn(example[\"target\"])\n",
    "    return id, airs, fgs, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CalibrationFn(beam.DoFn):\n",
    "    \"\"\"Perfroms the raw data calibration.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        cut_inf: int,\n",
    "        cut_sup: int,\n",
    "        mask: bool,\n",
    "        corr: bool,\n",
    "        dark: bool,\n",
    "        flat: bool,\n",
    "        binning: int | None = None,\n",
    "    ):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            cut_inf (int): Images cropping range lower limit.\n",
    "            cut_sup (int): Images cropping range upper limit.\n",
    "            binning (int | None): Number of images to bin together.\n",
    "            mask (bool): Wheteher to mask dead and hot pixels.\n",
    "            corr (bool): Wheteher to apply linear correction.\n",
    "            dark (bool): Whether to apply current dark correction.\n",
    "            flat (bool): Whether to apply flat pixels correction.\n",
    "        \"\"\"\n",
    "\n",
    "        self.CUT_INF = cut_inf\n",
    "        self.CUT_SUP = cut_sup\n",
    "        self.BINNING = binning\n",
    "        self.MASK = mask\n",
    "        self.CORR = corr\n",
    "        self.DARK = dark\n",
    "        self.FLAT = flat\n",
    "\n",
    "    def process(self, element):\n",
    "        id, uris = element\n",
    "        airs_data, fgs_data, info_data = self._load_data(id, uris)\n",
    "        airs_signal = self._calibrate_airs_data(airs_data, info_data)\n",
    "        airs_signal = tf.convert_to_tensor(airs_signal.reshape(1, *airs_signal.shape))\n",
    "        fgs_signal = self._calibrate_fgs_data(fgs_data, info_data)\n",
    "        fgs_signal = tf.convert_to_tensor(fgs_signal.reshape(1, *fgs_signal.shape))\n",
    "        labels = (\n",
    "            np.array([np.nan]) if info_data[\"labels\"] is None else info_data[\"labels\"]\n",
    "        )\n",
    "        labels = tf.convert_to_tensor(labels.reshape(1, *labels.shape))\n",
    "        return [(int(id), airs_signal, fgs_signal, labels)]\n",
    "\n",
    "    def _calibrate_airs_data(self, data: dict[pd.DataFrame], info: dict[pd.DataFrame]):\n",
    "        signal = self._adc_revert(\n",
    "            data[\"signal\"], info[\"airs_gain\"], info[\"airs_offset\"]\n",
    "        )\n",
    "        dt = info[\"airs_it\"]\n",
    "        dt[1::2] += 0.1\n",
    "        signal = signal[:, :, self.CUT_INF : self.CUT_SUP]\n",
    "        if self.MASK:\n",
    "            signal = self._mask_hot_dead(signal, data[\"dead\"], data[\"dark\"])\n",
    "        if self.CORR:\n",
    "            signal = self._apply_linear_corr(data[\"linear_corr\"], signal)\n",
    "        if self.DARK:\n",
    "            signal = self._clean_dark_current(signal, data[\"dead\"], data[\"dark\"], dt)\n",
    "        signal = self._get_cds(signal)\n",
    "        if self.BINNING:\n",
    "            signal = self._bin_obs(signal, self.BINNING)\n",
    "        else:\n",
    "            signal = signal.transpose(0, 2, 1)\n",
    "        if self.FLAT:\n",
    "            signal = self._correct_flat_field(data[\"flat\"], data[\"dead\"], signal)\n",
    "        return signal\n",
    "\n",
    "    def _calibrate_fgs_data(self, data: dict[pd.DataFrame], info: dict[pd.DataFrame]):\n",
    "        signal = self._adc_revert(data[\"signal\"], info[\"fgs_gain\"], info[\"fgs_offset\"])\n",
    "        dt = np.ones(len(signal)) * 0.1\n",
    "        dt[1::2] += 0.1\n",
    "        if self.MASK:\n",
    "            signal = self._mask_hot_dead(signal, data[\"dead\"], data[\"dark\"])\n",
    "        if self.CORR:\n",
    "            signal = self._apply_linear_corr(data[\"linear_corr\"], signal)\n",
    "        if self.DARK:\n",
    "            signal = self._clean_dark_current(signal, data[\"dead\"], data[\"dark\"], dt)\n",
    "        signal = self._get_cds(signal)\n",
    "        if self.BINNING:\n",
    "            signal = self._bin_obs(signal, self.BINNING * 12)\n",
    "        else:\n",
    "            signal = signal.transpose(0, 2, 1)\n",
    "        if self.FLAT:\n",
    "            signal = self._correct_flat_field(data[\"flat\"], data[\"dead\"], signal)\n",
    "        return signal\n",
    "\n",
    "    def _adc_revert(self, signal: np.ndarray, gain: float, offset: float) -> np.ndarray:\n",
    "        \"\"\"Revert pixel voltage from ADC.\n",
    "\n",
    "        Args:\n",
    "            signal (np.ndarray): ADC converted signal integer.\n",
    "            gain (float): ADC gain error.\n",
    "            offset (float): ADC offset error.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Pixel voltages.\n",
    "        \"\"\"\n",
    "        signal = signal.astype(np.float64)\n",
    "        signal /= gain\n",
    "        signal += offset\n",
    "        return signal\n",
    "\n",
    "    def _mask_hot_dead(\n",
    "        self, signal: np.ndarray, dead: np.ndarray, dark: np.ndarray\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Mask dead and hot pixels so that they won't be take in account in corrections.\n",
    "\n",
    "        Args:\n",
    "            signal (np.ndarray): Pixel voltage signal.\n",
    "            dead (np.ndarray): Dead pixels.\n",
    "            dark (np.ndarray): Dark pixels.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Pixel voltages with dead pixels masked.\n",
    "        \"\"\"\n",
    "        hot = sigma_clip(dark, sigma=5, maxiters=5).mask\n",
    "        hot = np.tile(hot, (signal.shape[0], 1, 1))\n",
    "        dead = np.tile(dead, (signal.shape[0], 1, 1))\n",
    "        signal = np.ma.masked_where(dead, signal)\n",
    "        signal = np.ma.masked_where(hot, signal)\n",
    "        return signal\n",
    "\n",
    "    def _apply_linear_corr(self, corr: np.ndarray, signal: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Fix non-linearity due to capacity leakage in the detector.\n",
    "\n",
    "        Args:\n",
    "            corr (np.ndarray): Correction coefficients\n",
    "            signal (np.ndarray): Signal to correct.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Corrected signal\n",
    "        \"\"\"\n",
    "        linear_corr = np.flip(corr, axis=0)\n",
    "        for x, y in itertools.product(range(signal.shape[1]), range(signal.shape[2])):\n",
    "            poli = np.poly1d(linear_corr[:, x, y])\n",
    "            signal[:, x, y] = poli(signal[:, x, y])\n",
    "        return signal\n",
    "\n",
    "    def _clean_dark_current(\n",
    "        self, signal: np.ndarray, dead: np.ndarray, dark: np.ndarray, dt: np.array\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Remove the accumulated charge due to dark current.\n",
    "\n",
    "        Args:\n",
    "            signal (np.ndarray): Signal to clean\n",
    "            dead (np.ndarray): Dead pixels.\n",
    "            dark (np.ndarray): Dark pixels.\n",
    "            dt (np.array): Short frames delay.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Cleaned signal.\n",
    "        \"\"\"\n",
    "        dark = np.ma.masked_where(dead, dark)\n",
    "        dark = np.tile(dark, (signal.shape[0], 1, 1))\n",
    "\n",
    "        signal -= dark * dt[:, np.newaxis, np.newaxis]\n",
    "        return signal\n",
    "\n",
    "    def _get_cds(self, signal: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Return the actual accumulated charge (a delta) due to the transit.\n",
    "\n",
    "        Args:\n",
    "            signal (np.ndarray): Signal.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: An image for one observation in the time (Time series observations).\n",
    "        \"\"\"\n",
    "        cds = signal[1::2, :, :] - signal[::2, :, :]\n",
    "        return cds\n",
    "\n",
    "    def _bin_obs(self, cds: np.ndarray, binning: int) -> np.ndarray:\n",
    "        \"\"\"Binnes cds time series together at the specified frequency.\n",
    "\n",
    "        Args:\n",
    "            cds (np.ndarray): CDS signal.\n",
    "            binning (int): Binning frequency.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: _description_\n",
    "        \"\"\"\n",
    "        cds_transposed = cds.transpose(0, 2, 1)\n",
    "        cds_binned = np.zeros(\n",
    "            (\n",
    "                cds_transposed.shape[0] // binning,\n",
    "                cds_transposed.shape[1],\n",
    "                cds_transposed.shape[2],\n",
    "            )\n",
    "        )\n",
    "        for i in range(cds_transposed.shape[0] // binning):\n",
    "            cds_binned[i, :, :] = np.sum(\n",
    "                cds_transposed[i * binning : (i + 1) * binning, :, :], axis=0\n",
    "            )\n",
    "        return cds_binned\n",
    "\n",
    "    def _correct_flat_field(\n",
    "        self, flat: np.ndarray, dead: np.ndarray, signal: np.ndarray\n",
    "    ):\n",
    "        \"\"\"Correction by calibrating on an uniform signal.\n",
    "\n",
    "        Args:\n",
    "            flat (np.ndarray): Flat signal\n",
    "            dead (np.ndarray): Dead pixels\n",
    "            signal (np.ndarray): CDS signal\n",
    "        \"\"\"\n",
    "\n",
    "        flat = flat.transpose(1, 0)\n",
    "        dead = dead.transpose(1, 0)\n",
    "        flat = np.ma.masked_where(dead, flat)\n",
    "        flat = np.tile(flat, (signal.shape[0], 1, 1))\n",
    "        signal = signal / flat\n",
    "        return signal\n",
    "\n",
    "    def _load_data(self, id: int, uris: List[str]):\n",
    "        \"\"\"\n",
    "        Load data from a list of URIs, reading the files as CSV or Parquet,\n",
    "        and organize them by type ('dark', 'dead', etc.) for AIRS and FGS.\n",
    "\n",
    "        Args:\n",
    "            id (int): Planet's id.\n",
    "            uris (List[str]): List of URIs to load data from.\n",
    "        \"\"\"\n",
    "        airs_data = {}\n",
    "        fgs_data = {}\n",
    "        info_data = {}\n",
    "        calib_data_types = [\n",
    "            \"dark\",\n",
    "            \"dead\",\n",
    "            \"flat\",\n",
    "            \"linear_corr\",\n",
    "            \"read\",\n",
    "            \"signal\",\n",
    "            \"labels\",\n",
    "            \"axis_info\",\n",
    "            \"adc_info\",\n",
    "        ]\n",
    "\n",
    "        def read_data(uri: str):\n",
    "            try:\n",
    "                return pd.read_csv(uri)\n",
    "            except UnicodeDecodeError:\n",
    "                return pd.read_parquet(uri)\n",
    "\n",
    "        for uri in uris:\n",
    "            df = read_data(uri)\n",
    "            for data_type in calib_data_types:\n",
    "                if data_type in uri:\n",
    "                    if \"AIRS\" in uri:\n",
    "                        if \"signal\" in uri:\n",
    "                            airs_data[data_type] = df.values.astype(np.float64).reshape(\n",
    "                                (df.shape[0], 32, 356)\n",
    "                            )\n",
    "                        elif \"linear_corr\" in uri:\n",
    "                            airs_data[data_type] = df.values.astype(np.float64).reshape(\n",
    "                                (6, 32, 356)\n",
    "                            )[:, :, self.CUT_INF : self.CUT_SUP]\n",
    "                        else:\n",
    "                            airs_data[data_type] = df.values.astype(np.float64).reshape(\n",
    "                                (32, 356)\n",
    "                            )[:, self.CUT_INF : self.CUT_SUP]\n",
    "                    elif \"FGS\" in uri:\n",
    "                        if \"signal\" in uri:\n",
    "                            fgs_data[data_type] = df.values.astype(np.float64).reshape(\n",
    "                                (df.shape[0], 32, 32)\n",
    "                            )\n",
    "                        elif \"linear_corr\" in uri:\n",
    "                            fgs_data[data_type] = df.values.astype(np.float64).reshape(\n",
    "                                (6, 32, 32)\n",
    "                            )\n",
    "                        else:\n",
    "                            fgs_data[data_type] = df.values.astype(np.float64).reshape(\n",
    "                                (32, 32)\n",
    "                            )\n",
    "                    else:\n",
    "                        try:\n",
    "                            info_data[data_type] = df.set_index(\"planet_id\").loc[\n",
    "                                int(id)\n",
    "                            ]\n",
    "                        except KeyError:\n",
    "                            if \"labels\" in uri:\n",
    "                                info_data[data_type] = None\n",
    "                            elif \"axis_info\" in uri:\n",
    "                                info_data[data_type] = df\n",
    "                            else:\n",
    "                                pass\n",
    "        info_data[\"fgs_gain\"] = info_data[\"adc_info\"][\"FGS1_adc_gain\"]\n",
    "        info_data[\"fgs_offset\"] = info_data[\"adc_info\"][\"FGS1_adc_offset\"]\n",
    "        info_data[\"airs_gain\"] = info_data[\"adc_info\"][\"AIRS-CH0_adc_gain\"]\n",
    "        info_data[\"airs_offset\"] = info_data[\"adc_info\"][\"AIRS-CH0_adc_offset\"]\n",
    "        if info_data[\"labels\"] is not None:\n",
    "            info_data[\"labels\"] = info_data[\"labels\"].values\n",
    "        info_data[\"airs_it\"] = (\n",
    "            info_data[\"axis_info\"][\"AIRS-CH0-integration_time\"].dropna().values\n",
    "        )\n",
    "        del info_data[\"adc_info\"], info_data[\"axis_info\"]\n",
    "        return airs_data, fgs_data, info_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombineDataFn(beam.CombineFn):\n",
    "    def create_accumulator(self):\n",
    "        return (None, [])\n",
    "\n",
    "    def add_input(self, accumulator, input):\n",
    "        _, bag = accumulator\n",
    "        if bag is None:\n",
    "            bag = []\n",
    "        bag.append(input)\n",
    "        return (None, bag)\n",
    "\n",
    "    def merge_accumulators(self, accumulators):\n",
    "        merge = []\n",
    "        for _, item in accumulators:\n",
    "            merge.extend(item)\n",
    "        return (None, merge)\n",
    "\n",
    "    def extract_output(self, merge):\n",
    "        _, merge = merge\n",
    "        return [merge]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_example(id: int, airs: tf.Tensor, fgs: tf.Tensor, target: tf.Tensor):\n",
    "    id_ft = tf.train.Feature(int64_list=tf.train.Int64List(value=[id]))\n",
    "    airs_ft = tf.train.Feature(\n",
    "        bytes_list=tf.train.BytesList(\n",
    "            value=[\n",
    "                tf.io.serialize_tensor(airs).numpy(),\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "    fgs_ft = tf.train.Feature(\n",
    "        bytes_list=tf.train.BytesList(\n",
    "            value=[\n",
    "                tf.io.serialize_tensor(fgs).numpy(),\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "    target_ft = tf.train.Feature(\n",
    "        bytes_list=tf.train.BytesList(value=[tf.io.serialize_tensor(target).numpy()])\n",
    "    )\n",
    "    features = tf.train.Features(\n",
    "        feature={\"id\": id_ft, \"airs\": airs_ft, \"fgs\": fgs_ft, \"target\": target_ft}\n",
    "    )\n",
    "    example = tf.train.Example(features=features)\n",
    "    return example.SerializeToString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset_to_tfrecords(element: list, path: str):\n",
    "    examples = element[0]\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: iter(examples),\n",
    "        # output signature important car sans ça TF essaye de concatener tous\n",
    "        # les éléments du tuple dans un unique tensor ce qui crée des erreurs.\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=None, dtype=tf.int64),\n",
    "            tf.TensorSpec(shape=None, dtype=tf.float64),\n",
    "            tf.TensorSpec(shape=None, dtype=tf.float64),\n",
    "            tf.TensorSpec(shape=None, dtype=tf.float64),\n",
    "        ),\n",
    "    )\n",
    "    dataset = dataset.map(\n",
    "        lambda id, airs, fgs, target: tf.py_function(\n",
    "            func=make_example, inp=[id, airs, fgs, target], Tout=tf.string\n",
    "        )\n",
    "    )\n",
    "    save_to_tfrecords(dataset, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "with beam.Pipeline() as pipeline:\n",
    "    uris = get_raw_data_uris(\"neurips-adc-bucket\", \"raw\")[:2]\n",
    "    outputs = (\n",
    "        pipeline\n",
    "        | \"Create initial values\" >> beam.Create(uris)\n",
    "        | \"Calibrate data\"\n",
    "        >> beam.ParDo(CalibrationFn(39, 321, False, False, False, False, 30))\n",
    "        | \"Create a data set from accumulators\" >> beam.CombineGlobally(CombineDataFn())\n",
    "        | \"Save dataset\"\n",
    "        >> beam.Map(\n",
    "            lambda x: save_dataset_to_tfrecords(\n",
    "                x, \"gs://neurips-adc-bucket/primary/beam_output.tfrecords\"\n",
    "            )\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_ds = read_tfrecord(\"gs://neurips-adc-bucket/primary/beam_output.tfrecords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_ds = loaded_ds.map(read_labeled_tfrecord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELED_TFREC_FORMAT = {\n",
    "    \"id\": tf.io.FixedLenFeature([], tf.int64),\n",
    "    \"arr1\": tf.io.FixedLenFeature([], tf.string),\n",
    "    \"arr2\": tf.io.FixedLenFeature([], tf.string),\n",
    "    \"arr3\": tf.io.FixedLenFeature([], tf.string),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PIPELINE OPTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neurips",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
