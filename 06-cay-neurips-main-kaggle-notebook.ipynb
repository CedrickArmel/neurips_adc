{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9644542,"sourceType":"datasetVersion","datasetId":5889896}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport matplotlib.pyplot as plt\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\n\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom tqdm.auto import tqdm\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-22T17:28:54.740656Z","iopub.execute_input":"2024-10-22T17:28:54.741184Z","iopub.status.idle":"2024-10-22T17:29:05.173925Z","shell.execute_reply.started":"2024-10-22T17:28:54.741139Z","shell.execute_reply":"2024-10-22T17:29:05.172835Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/neurips-adc-challenge-2024-calibrated-tfrecord/primary-neurips-dataset-20241015205628-c1d1f1m1b30.tfrecords\n","output_type":"stream"}]},{"cell_type":"code","source":"primary_dataset = tf.data.TFRecordDataset(\n    \"/kaggle/input/neurips-adc-challenge-2024-calibrated-tfrecord/primary-neurips-dataset-20241015205628-c1d1f1m1b30.tfrecords\")","metadata":{"execution":{"iopub.status.busy":"2024-10-22T17:29:19.387488Z","iopub.execute_input":"2024-10-22T17:29:19.388602Z","iopub.status.idle":"2024-10-22T17:29:19.452515Z","shell.execute_reply.started":"2024-10-22T17:29:19.388550Z","shell.execute_reply":"2024-10-22T17:29:19.451500Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Decode`tf.train.Examples` in the `TFRecord`s","metadata":{}},{"cell_type":"code","source":"def decode_fn(proto):\n    features_formats = {\n        \"id\": tf.io.FixedLenFeature([], tf.int64),\n        \"airs\": tf.io.FixedLenFeature([], tf.string),\n        \"fgs\": tf.io.FixedLenFeature([], tf.string),\n        \"target\": tf.io.FixedLenFeature([], tf.string),\n    }\n    example = tf.io.parse_single_example(proto, features_formats)\n    pid = example[\"id\"]\n    airs = tf.io.parse_tensor(example[\"airs\"], tf.float64)\n    fgs = tf.io.parse_tensor(example[\"fgs\"], tf.float64)\n    target = tf.io.parse_tensor(example[\"target\"], tf.float64)\n    return pid, airs, fgs, target","metadata":{"execution":{"iopub.status.busy":"2024-10-22T17:29:34.812405Z","iopub.execute_input":"2024-10-22T17:29:34.812862Z","iopub.status.idle":"2024-10-22T17:29:34.820802Z","shell.execute_reply.started":"2024-10-22T17:29:34.812824Z","shell.execute_reply":"2024-10-22T17:29:34.819351Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def concatenate_signals(proto):\n    _, airs, fgs, target = decode_fn(proto)\n    signal = np.concatenate(\n        [airs.numpy(), fgs.numpy().sum(axis=2)[:,:, np.newaxis,:]], axis=2\n    )\n    signal = tf.convert_to_tensor(signal)\n    return signal, target","metadata":{"execution":{"iopub.status.busy":"2024-10-22T17:29:42.985595Z","iopub.execute_input":"2024-10-22T17:29:42.986043Z","iopub.status.idle":"2024-10-22T17:29:42.992693Z","shell.execute_reply.started":"2024-10-22T17:29:42.986005Z","shell.execute_reply":"2024-10-22T17:29:42.991329Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"dataset = primary_dataset.prefetch(\n    buffer_size=tf.data.AUTOTUNE).map(lambda example: tf.py_function(\n    func=concatenate_signals, inp=[example],\n    Tout=[\n        tf.TensorSpec(shape=None, dtype=tf.float64),\n        tf.TensorSpec(shape=None, dtype=tf.float64)\n    ]))","metadata":{"execution":{"iopub.status.busy":"2024-10-22T17:29:48.514938Z","iopub.execute_input":"2024-10-22T17:29:48.516175Z","iopub.status.idle":"2024-10-22T17:29:48.568381Z","shell.execute_reply.started":"2024-10-22T17:29:48.516126Z","shell.execute_reply":"2024-10-22T17:29:48.567140Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# x, y = batch\n# x = x.numpy().reshape(-1, 187, 283, 32)\n# x = x.sum(axis=3).sum(axis=2) / (x.sum(axis=3).mean(axis=1).mean(axis=1)[: , np.newaxis] / 30)  #  /30 because of binning","metadata":{"execution":{"iopub.status.busy":"2024-10-22T15:50:55.817418Z","iopub.execute_input":"2024-10-22T15:50:55.817864Z","iopub.status.idle":"2024-10-22T15:50:55.854282Z","shell.execute_reply.started":"2024-10-22T15:50:55.817822Z","shell.execute_reply":"2024-10-22T15:50:55.853162Z"},"trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"code","source":"# img = np.array([[1, 2, 3],[4, 5, 6],[7, 8, 9]])\n# planet = np.array([img + 1, img + 2])\n# ds = np.array([planet + 1, planet + 2])","metadata":{"execution":{"iopub.status.busy":"2024-10-22T14:28:13.454361Z","iopub.execute_input":"2024-10-22T14:28:13.454880Z","iopub.status.idle":"2024-10-22T14:28:13.461744Z","shell.execute_reply.started":"2024-10-22T14:28:13.454836Z","shell.execute_reply":"2024-10-22T14:28:13.460343Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"np_signal_dataset = None\nnp_target_dataset = None\nfor obs in tqdm(iter(dataset)):\n    try:\n        if np_signal_dataset is None:\n            np_signal_dataset = obs[0].numpy()\n            np_target_dataset = obs[1].numpy()\n        else:\n            np_signal_dataset = np.concatenate([np_signal_dataset, obs[0].numpy()])\n            np_target_dataset = np.concatenate([np_target_dataset, obs[1].numpy()])\n    except ValueError:\n        pass","metadata":{"execution":{"iopub.status.busy":"2024-10-22T17:30:00.713976Z","iopub.execute_input":"2024-10-22T17:30:00.715240Z","iopub.status.idle":"2024-10-22T17:50:38.191971Z","shell.execute_reply.started":"2024-10-22T17:30:00.715190Z","shell.execute_reply":"2024-10-22T17:50:38.190631Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c45fb499518e4ad6867e8db1558b1923"}},"metadata":{}}]},{"cell_type":"code","source":"def make_example(x: tf.Tensor, y: tf.Tensor):\n    x_ft = tf.train.Feature(\n        bytes_list=tf.train.BytesList(\n            value=[\n                tf.io.serialize_tensor(x).numpy(),\n            ]\n        )\n    )\n    y_ft = tf.train.Feature(\n        bytes_list=tf.train.BytesList(\n            value=[\n                tf.io.serialize_tensor(y).numpy(),\n            ]\n        )\n    )\n\n    features = tf.train.Features(\n        feature={\"x\": x_ft, \"y\": y_ft}\n    )\n    example = tf.train.Example(features=features)\n    return example.SerializeToString()","metadata":{"execution":{"iopub.status.busy":"2024-10-22T17:53:08.406428Z","iopub.execute_input":"2024-10-22T17:53:08.407657Z","iopub.status.idle":"2024-10-22T17:53:08.414703Z","shell.execute_reply.started":"2024-10-22T17:53:08.407594Z","shell.execute_reply":"2024-10-22T17:53:08.413475Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def save_to_tfrecords(dataset: tf.data.Dataset, path: str) -> None:\n    \"\"\"Save a dataset to TFRecord.\n\n    Args:\n        dataset (tf.data.Dataset): A Tensorflow Dataset\n        path (str): Path to save the dataset\n    \"\"\"\n    with tf.io.TFRecordWriter(path) as file_writer:\n        for record in dataset:\n            try:\n                file_writer.write(record.numpy())\n            except AttributeError:\n                file_writer.write(record)","metadata":{"execution":{"iopub.status.busy":"2024-10-22T17:53:12.297270Z","iopub.execute_input":"2024-10-22T17:53:12.298459Z","iopub.status.idle":"2024-10-22T17:53:12.305338Z","shell.execute_reply.started":"2024-10-22T17:53:12.298403Z","shell.execute_reply":"2024-10-22T17:53:12.303782Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"train_dataset = tf.data.Dataset.from_tensors((np_signal_dataset, np_target_dataset))\ntrain_dataset = train_dataset.map(lambda x, y: tf.py_function(\n    func=make_example, inp=[x, y],\n    Tout=tf.string))\n","metadata":{"execution":{"iopub.status.busy":"2024-10-22T17:53:17.060995Z","iopub.execute_input":"2024-10-22T17:53:17.062033Z","iopub.status.idle":"2024-10-22T17:53:37.314481Z","shell.execute_reply.started":"2024-10-22T17:53:17.061986Z","shell.execute_reply":"2024-10-22T17:53:37.313265Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"save_to_tfrecords(train_dataset, \"/kaggle/working/primary-neurips-dataset-20241015205628-c1d1f1m1b30-train.tfrecords\")","metadata":{"execution":{"iopub.status.busy":"2024-10-22T17:53:43.927374Z","iopub.execute_input":"2024-10-22T17:53:43.928247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Dans la suite on calcule un bruit blanc: La somme de tous les signaux par image que l'on divise par un signale blanc moyen par planete","metadata":{}},{"cell_type":"code","source":"x = np_signal_dataset.sum(axis=3).sum(axis=2) / (np_signal_dataset.sum(axis=3).mean(axis=1).mean(axis=1)[: , np.newaxis] / 30)\ny = np_target_dataset.mean(axis=1)\ndel np_signal_dataset, np_target_dataset","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2, shuffle=True, random_state=7)\nscaler_xt = MinMaxScaler()\nscaler_xte = MinMaxScaler()\nscaler_yt = MinMaxScaler()\nscaler_yte = MinMaxScaler()\n\nx_train_scalled = scaler_xt.fit_transform(x_train)\nx_test_scalled = scaler_xte.fit_transform(x_test)\ny_train_scalled = scaler_yt.fit_transform(y_train)\ny_test_scalled = scaler_yte.fit_transform(y_test)","metadata":{},"execution_count":null,"outputs":[]}]}